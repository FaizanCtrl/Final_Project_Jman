{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load raw data from the RAW folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data_file = '../RAW/data/users.csv'  # Path to your CSV file in the RAW folder\n",
    "users_raw = pd.read_csv(raw_data_file)\n",
    "departments_raw_file = '../RAW/data/departments.csv'  # Path to your CSV file for departments\n",
    "departments_raw = pd.read_csv(departments_raw_file)\n",
    "teams_raw_file = '../RAW/data/teams.csv'  # Path to your CSV file for teams\n",
    "# Load the raw team data\n",
    "teams_raw = pd.read_csv(teams_raw_file)\n",
    "# load the course data\n",
    "courses_raw_file = '../RAW/data/courses.csv'  # Path to your CSV file for courses\n",
    "courses_raw = pd.read_csv(courses_raw_file)\n",
    "learning_materials_raw_file = '../RAW/data/learning_materials.csv'  # Path to your CSV file for learning materials\n",
    "# Load the raw learning materials data\n",
    "learning_materials_raw = pd.read_csv(learning_materials_raw_file)\n",
    "\n",
    "\n",
    "# Load raw data from the RAW folder\n",
    "user_progress_raw_file = '../RAW/data/user_progress.csv'  # Path to your CSV file for user progress\n",
    "quizzes_raw_file = '../RAW/data/quizzes.csv'  # Path to your CSV file for quizzes\n",
    "questions_raw_file = '../RAW/data/questions.csv'  # Path to your CSV file for questions\n",
    "results_raw_file = '../RAW/data/results.csv'  # Path to your CSV file for results\n",
    "posts_raw_file = '../RAW/data/posts.csv'  # Path to your CSV file for posts\n",
    "feedbacks_raw_file = '../RAW/data/feedbacks.csv'  # Path to your CSV file for feedback\n",
    "user_sessions_raw_file = '../RAW/data/user_sessions.csv'  # Path to your CSV file for user sessions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create PREP folder if it doesn't exist\n",
    "os.makedirs('./data', exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning: Remove duplicates and NaN values for users and load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "users_cleaned = users_raw.drop_duplicates(subset=['email'])\n",
    "users_cleaned = users_cleaned.dropna(subset=['name', 'email'])\n",
    "\n",
    "# some transformation\n",
    "users_cleaned['name'] = users_cleaned['name'].str.title()  # Capitalize names\n",
    "users_cleaned['email'] = users_cleaned['email'].str.lower()  # Lowercase \n",
    "\n",
    "\n",
    "# loading to prep layer\n",
    "prep_file_path = './data/prep_users.csv'\n",
    "users_cleaned.to_csv(prep_file_path, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## For department and team"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "departments_cleaned = departments_raw.drop_duplicates(subset=['name'])\n",
    "departments_cleaned = departments_cleaned.dropna(subset=['name'])\n",
    "\n",
    "\n",
    "prep_departments_file_path = './data/prep_departments.csv'\n",
    "departments_cleaned.to_csv(prep_departments_file_path, index=False)\n",
    "\n",
    "# Cleaning for Teams\n",
    "teams_cleaned = teams_raw.drop_duplicates(subset=['name'])\n",
    "teams_cleaned = teams_cleaned.dropna(subset=['name', 'departmentId'])\n",
    "\n",
    "# Optional: Check if departmentId exists in the departments dataset\n",
    "# This step is to ensure referential integrity (if needed)\n",
    "if 'departmentId' in teams_cleaned.columns:\n",
    "    valid_departments = departments_cleaned['id'].unique()\n",
    "    teams_cleaned = teams_cleaned[teams_cleaned['departmentId'].isin(valid_departments)]\n",
    "\n",
    "# Saving cleaned teams data\n",
    "prep_teams_file_path = './data/prep_teams.csv'\n",
    "teams_cleaned.to_csv(prep_teams_file_path, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## For course and learning material"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "courses_cleaned = courses_raw.drop_duplicates(subset=['name'])\n",
    "courses_cleaned = courses_cleaned.dropna(subset=['name', 'content', 'teamId'])\n",
    "\n",
    "# Check if teamId exists in the teams dataset\n",
    "teams_file_path = './data/prep_teams.csv'\n",
    "if 'teamId' in courses_cleaned.columns:\n",
    "    teams_df = pd.read_csv(teams_file_path)\n",
    "    valid_teams = teams_df['id'].unique()\n",
    "    courses_cleaned = courses_cleaned[courses_cleaned['teamId'].isin(valid_teams)]\n",
    "\n",
    "# Saving cleaned course data\n",
    "prep_courses_file_path = './data/prep_courses.csv'\n",
    "courses_cleaned.to_csv(prep_courses_file_path, index=False)\n",
    "\n",
    "\n",
    "# Cleaning for Learning Materials\n",
    "learning_materials_cleaned = learning_materials_raw.drop_duplicates(subset=['title'])\n",
    "learning_materials_cleaned = learning_materials_cleaned.dropna(subset=['title', 'type', 'content', 'courseId'])\n",
    "\n",
    "# Check if courseId exists in the courses dataset\n",
    "if 'courseId' in learning_materials_cleaned.columns:\n",
    "    valid_courses = courses_cleaned['id'].unique()\n",
    "    learning_materials_cleaned = learning_materials_cleaned[learning_materials_cleaned['courseId'].isin(valid_courses)]\n",
    "\n",
    "# Saving cleaned learning materials data\n",
    "prep_learning_materials_file_path = './data/prep_learning_materials.csv'\n",
    "learning_materials_cleaned.to_csv(prep_learning_materials_file_path, index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## For user progress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User Progress data cleaned and saved to ./data/prep_user_progress.csv successfully.\n"
     ]
    }
   ],
   "source": [
    "user_progress_raw = pd.read_csv(user_progress_raw_file)\n",
    "\n",
    "# Cleaning UserProgress\n",
    "user_progress_cleaned = user_progress_raw.drop_duplicates(subset=['userId', 'learningMaterialId'])\n",
    "user_progress_cleaned = user_progress_cleaned.dropna(subset=['userId', 'learningMaterialId'])\n",
    "\n",
    "# Convert completedAt to datetime\n",
    "user_progress_cleaned['completedAt'] = pd.to_datetime(user_progress_cleaned['completedAt'], errors='coerce')\n",
    "\n",
    "# Saving cleaned user progress data\n",
    "prep_user_progress_file_path = './data/prep_user_progress.csv'\n",
    "user_progress_cleaned.to_csv(prep_user_progress_file_path, index=False)\n",
    "print(f\"User Progress data cleaned and saved to {prep_user_progress_file_path} successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## For quiz data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quizzes data cleaned and saved to ./data/prep_quizzes.csv successfully.\n"
     ]
    }
   ],
   "source": [
    "quizzes_raw = pd.read_csv(quizzes_raw_file)\n",
    "\n",
    "# Cleaning Quizzes\n",
    "quizzes_cleaned = quizzes_raw.drop_duplicates(subset=['courseId'])\n",
    "quizzes_cleaned = quizzes_cleaned.dropna(subset=['courseId'])\n",
    "\n",
    "# Check if courseId exists in the courses dataset\n",
    "courses_df = pd.read_csv('./data/prep_courses.csv')\n",
    "valid_courses = courses_df['id'].unique()\n",
    "quizzes_cleaned = quizzes_cleaned[quizzes_cleaned['courseId'].isin(valid_courses)]\n",
    "\n",
    "# Saving cleaned quizzes data\n",
    "prep_quizzes_file_path = './data/prep_quizzes.csv'\n",
    "quizzes_cleaned.to_csv(prep_quizzes_file_path, index=False)\n",
    "print(f\"Quizzes data cleaned and saved to {prep_quizzes_file_path} successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## For questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Questions data cleaned and saved to ./data/prep_questions.csv successfully.\n"
     ]
    }
   ],
   "source": [
    "questions_raw = pd.read_csv(questions_raw_file)\n",
    "\n",
    "# Cleaning Questions\n",
    "questions_cleaned = questions_raw.drop_duplicates(subset=['quizId'])\n",
    "questions_cleaned = questions_cleaned.dropna(subset=['quizId', 'questionText', 'answerA', 'answerB', 'answerC', 'answerD', 'correctAnswer'])\n",
    "\n",
    "# Check if quizId exists in the quizzes dataset\n",
    "quizzes_df = pd.read_csv('./data/prep_quizzes.csv')\n",
    "valid_quizzes = quizzes_df['id'].unique()\n",
    "questions_cleaned = questions_cleaned[questions_cleaned['quizId'].isin(valid_quizzes)]\n",
    "\n",
    "# Saving cleaned questions data\n",
    "prep_questions_file_path = './data/prep_questions.csv'\n",
    "questions_cleaned.to_csv(prep_questions_file_path, index=False)\n",
    "print(f\"Questions data cleaned and saved to {prep_questions_file_path} successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## For results data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results data cleaned and saved to ./data/prep_results.csv successfully.\n"
     ]
    }
   ],
   "source": [
    "results_raw = pd.read_csv(results_raw_file)\n",
    "\n",
    "# Cleaning Results\n",
    "results_cleaned = results_raw.drop_duplicates(subset=['userId', 'quizId'])\n",
    "results_cleaned = results_cleaned.dropna(subset=['userId', 'quizId', 'score', 'totalScore'])\n",
    "\n",
    "# Check if userId exists in the users dataset\n",
    "users_df = pd.read_csv('./data/prep_users.csv')\n",
    "valid_users = users_df['id'].unique()\n",
    "results_cleaned = results_cleaned[results_cleaned['userId'].isin(valid_users)]\n",
    "\n",
    "# Check if quizId exists in the quizzes dataset\n",
    "quizzes_df = pd.read_csv('./data/prep_quizzes.csv')\n",
    "valid_quizzes = quizzes_df['id'].unique()\n",
    "results_cleaned = results_cleaned[results_cleaned['quizId'].isin(valid_quizzes)]\n",
    "\n",
    "# Saving cleaned results data\n",
    "prep_results_file_path = './data/prep_results.csv'\n",
    "results_cleaned.to_csv(prep_results_file_path, index=False)\n",
    "print(f\"Results data cleaned and saved to {prep_results_file_path} successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## For posts data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Posts data cleaned and saved to ./data/prep_posts.csv successfully.\n"
     ]
    }
   ],
   "source": [
    "posts_raw = pd.read_csv(posts_raw_file)\n",
    "\n",
    "# Cleaning Posts\n",
    "posts_cleaned = posts_raw.drop_duplicates(subset=['userId', 'courseId'])\n",
    "posts_cleaned = posts_cleaned.dropna(subset=['content', 'userId', 'courseId'])\n",
    "\n",
    "# Optional: Check if userId exists in the users dataset\n",
    "users_df = pd.read_csv('./data/prep_users.csv')\n",
    "valid_users = users_df['id'].unique()\n",
    "posts_cleaned = posts_cleaned[posts_cleaned['userId'].isin(valid_users)]\n",
    "\n",
    "# Convert createdAt to datetime\n",
    "posts_cleaned['createdAt'] = pd.to_datetime(posts_cleaned['createdAt'], errors='coerce')\n",
    "\n",
    "\n",
    "# Optional: Check if courseId exists in the courses dataset\n",
    "courses_df = pd.read_csv('./data/prep_courses.csv')\n",
    "valid_courses = courses_df['id'].unique()\n",
    "posts_cleaned = posts_cleaned[posts_cleaned['courseId'].isin(valid_courses)]\n",
    "\n",
    "# Saving cleaned posts data\n",
    "prep_posts_file_path = './data/prep_posts.csv'\n",
    "posts_cleaned.to_csv(prep_posts_file_path, index=False)\n",
    "print(f\"Posts data cleaned and saved to {prep_posts_file_path} successfully.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## For feedbacks data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feedback data cleaned and saved to ./data/prep_feedbacks.csv successfully.\n"
     ]
    }
   ],
   "source": [
    "feedbacks_raw = pd.read_csv(feedbacks_raw_file)\n",
    "\n",
    "# Cleaning Feedback\n",
    "feedbacks_cleaned = feedbacks_raw.drop_duplicates(subset=['userId', 'courseId'])\n",
    "feedbacks_cleaned = feedbacks_cleaned.dropna(subset=['content', 'rating', 'userId', 'courseId'])\n",
    "\n",
    "# Convert createdAt to datetime\n",
    "feedbacks_cleaned['createdAt'] = pd.to_datetime(feedbacks_cleaned['createdAt'], errors='coerce')\n",
    "\n",
    "# Optional: Check if userId exists in the users dataset\n",
    "users_df = pd.read_csv('./data/prep_users.csv')\n",
    "valid_users = users_df['id'].unique()\n",
    "feedbacks_cleaned = feedbacks_cleaned[feedbacks_cleaned['userId'].isin(valid_users)]\n",
    "\n",
    "# Optional: Check if courseId exists in the courses dataset\n",
    "courses_df = pd.read_csv('./data/prep_courses.csv')\n",
    "valid_courses = courses_df['id'].unique()\n",
    "feedbacks_cleaned = feedbacks_cleaned[feedbacks_cleaned['courseId'].isin(valid_courses)]\n",
    "\n",
    "# Saving cleaned feedbacks data\n",
    "prep_feedbacks_file_path = './data/prep_feedbacks.csv'\n",
    "feedbacks_cleaned.to_csv(prep_feedbacks_file_path, index=False)\n",
    "print(f\"Feedback data cleaned and saved to {prep_feedbacks_file_path} successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## For user session data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User Sessions data cleaned and saved to ./data/prep_user_sessions.csv successfully.\n"
     ]
    }
   ],
   "source": [
    "user_sessions_raw = pd.read_csv(user_sessions_raw_file)\n",
    "\n",
    "# Cleaning User Sessions\n",
    "user_sessions_cleaned = user_sessions_raw.drop_duplicates(subset=['userId'])\n",
    "user_sessions_cleaned = user_sessions_cleaned.dropna(subset=['userId', 'duration'])\n",
    "\n",
    "# Optional: Check if userId exists in the users dataset\n",
    "users_df = pd.read_csv('./data/prep_users.csv')\n",
    "valid_users = users_df['id'].unique()\n",
    "user_sessions_cleaned = user_sessions_cleaned[user_sessions_cleaned['userId'].isin(valid_users)]\n",
    "\n",
    "# Saving cleaned user sessions data\n",
    "prep_user_sessions_file_path = './data/prep_user_sessions.csv'\n",
    "user_sessions_cleaned.to_csv(prep_user_sessions_file_path, index=False)\n",
    "print(f\"User Sessions data cleaned and saved to {prep_user_sessions_file_path} successfully.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
